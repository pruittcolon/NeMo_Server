# WhisperServer REFACTORED - All-in-One Dockerfile
# Single container with Python backend + Node.js frontend + GPU support
#
# Features:
# - FastAPI backend on port 8000
# - Next.js web UI on port 3000
# - GPU-accelerated ASR (Parakeet) + Gemma
# - All dependencies in one image

FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04

# Set working directory
WORKDIR /app

# ============================================================================
# System Dependencies
# ============================================================================
RUN apt-get update && apt-get install -y \
    # Python
    python3.10 \
    python3-pip \
    python3.10-dev \
    build-essential \
    cmake \
    git \
    # Audio processing
    ffmpeg \
    libsndfile1 \
    libsndfile1-dev \
    # Node.js for Next.js frontend
    curl \
    wget \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install Node.js 18.x (LTS)
RUN curl -fsSL https://deb.nodesource.com/setup_18.x | bash - && \
    apt-get install -y nodejs && \
    rm -rf /var/lib/apt/lists/*

# Verify installations
RUN python3.10 --version && \
    node --version && \
    npm --version

# ============================================================================
# Python Dependencies
# ============================================================================

# Upgrade pip
RUN python3.10 -m pip install --upgrade pip setuptools wheel

# Copy requirements
COPY requirements.txt /app/requirements.txt

# Set explicit CUDA paths for proper CMake detection and linking
ENV CUDA_PATH=/usr/local/cuda-12.1
ENV CUDAToolkit_ROOT=/usr/local/cuda-12.1
ENV CUDA_HOME=/usr/local/cuda-12.1
ENV LD_LIBRARY_PATH=/usr/local/cuda-12.1/targets/x86_64-linux/lib:/usr/local/cuda-12.1/lib64:${LD_LIBRARY_PATH}
ENV PATH=/usr/local/cuda-12.1/bin:${PATH}

# Install llama-cpp-python with CUDA support
# CRITICAL: Add stubs/ to LD_LIBRARY_PATH during build for libcuda.so.1 linking
# CUDA 12.x moved libraries from lib64 to targets/x86_64-linux/lib
# Reference: https://github.com/NVIDIA/nvidia-docker/issues/1632
RUN LD_LIBRARY_PATH=/usr/local/cuda-12.1/targets/x86_64-linux/lib/stubs:/usr/local/cuda-12.1/targets/x86_64-linux/lib:${LD_LIBRARY_PATH} \
    CMAKE_ARGS="-DGGML_CUDA=on \
    -DCUDAToolkit_ROOT=/usr/local/cuda-12.1 \
    -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.1/bin/nvcc \
    -DCMAKE_LIBRARY_PATH=/usr/local/cuda-12.1/targets/x86_64-linux/lib" \
    FORCE_CMAKE=1 \
    CUDACXX=/usr/local/cuda-12.1/bin/nvcc \
    pip install llama-cpp-python==0.2.90 --force-reinstall --no-cache-dir --verbose

# IMPORTANT: LD_LIBRARY_PATH for runtime does NOT include stubs (real driver injected at runtime)

# Verify llama-cpp-python CUDA support immediately
RUN python3.10 -c "import llama_cpp; print('✓ llama-cpp-python:', llama_cpp.__version__)" && \
    python3.10 -c "import llama_cpp; assert llama_cpp.llama_supports_gpu_offload(), 'CUDA support missing!'; print('✓ CUDA support: enabled')"

# Install remaining Python requirements (excluding llama-cpp-python)
RUN grep -v "llama-cpp-python" requirements.txt > /tmp/requirements_no_llama.txt && \
    pip install --no-cache-dir -r /tmp/requirements_no_llama.txt

# ============================================================================
# Backend Setup
# ============================================================================

# Copy original src for config.py dependency
COPY _original_src/config.py /app/src/config.py
COPY _original_src/__init__.py /app/src/__init__.py

# Copy refactored backend source
COPY src/ /app/REFACTORED_SRC/
RUN mkdir -p /app/REFACTORED_SRC/models \
             /app/REFACTORED_SRC/services \
             /app/REFACTORED_SRC/utils

# ============================================================================
# Frontend Setup (HTML Only - No Node.js build needed)
# ============================================================================

# Copy HTML frontend (served by FastAPI at /ui/)
COPY frontend_html/ /app/REFACTORED_SRC/frontend_html/

# ============================================================================
# Application Structure
# ============================================================================

WORKDIR /app

# Create necessary directories
RUN mkdir -p /app/models \
             /app/instance \
             /app/logs \
             /app/instance/uploads

# ============================================================================
# Environment Configuration
# ============================================================================

# Python paths
ENV PYTHONPATH=/app:/app/src:/app/REFACTORED_SRC
ENV PYTHONUNBUFFERED=1

# GPU configuration
ENV CUDA_VISIBLE_DEVICES=0
ENV NVIDIA_VISIBLE_DEVICES=0

# ============================================================================
# Ports
# ============================================================================

# FastAPI Backend (also serves HTML UI at /ui/)
EXPOSE 8000

# ============================================================================
# Health Check
# ============================================================================

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=5 \
    CMD curl -f http://localhost:8000/health || exit 1

# ============================================================================
# Startup Script
# ============================================================================

# Copy startup script
COPY scripts/start_all.sh /app/start_all.sh
RUN chmod +x /app/start_all.sh

# Start both services
CMD ["/app/start_all.sh"]
