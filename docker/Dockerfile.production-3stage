# ==============================================================================
# PRODUCTION DOCKERFILE FOR WHISPERSERVER WITH CUDA-ENABLED LLAMA-CPP-PYTHON
# ==============================================================================
# 
# This is a 3-stage build:
#   1. preflight    - Quick CUDA link test (seconds)
#   2. wheel-builder - Build llama-cpp-python==0.2.90 with CUDA (20 mins)
#   3. runtime      - Final production container with GPU verification
#
# Target GPU: NVIDIA GeForce GTX 1660 Ti (Compute Capability 7.5)
# CUDA Version: 12.1.1
# Python: 3.10
# ==============================================================================

# ==============================================================================
# STAGE 1: PREFLIGHT - Quick CUDA Link Test
# ==============================================================================
FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04 AS preflight

LABEL stage="preflight"
LABEL description="Fast CUDA linker verification before long build"

# Install minimal build tools
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    cmake \
    ninja-build \
    && rm -rf /var/lib/apt/lists/*

# Create test program that uses CUDA Driver API (same symbols that fail during llama-cpp-python build)
RUN cat > /tmp/link_test.cu << 'EOF'
#include <cuda.h>
#include <stdio.h>

int main() {
    CUresult res;
    int deviceCount = 0;
    
    // These are the exact symbols that cause "undefined reference" during llama-cpp-python linking
    res = cuInit(0);
    if (res != CUDA_SUCCESS) {
        printf("cuInit failed\n");
        return 1;
    }
    
    res = cuDeviceGetCount(&deviceCount);
    if (res != CUDA_SUCCESS) {
        printf("cuDeviceGetCount failed\n");
        return 1;
    }
    
    // Test other problematic symbols
    CUdevice device;
    if (deviceCount > 0) {
        res = cuDeviceGet(&device, 0);
        
        int major, minor;
        cuDeviceGetAttribute(&major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device);
        cuDeviceGetAttribute(&minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, device);
    }
    
    printf("PREFLIGHT SUCCESS: CUDA link test passed\n");
    return 0;
}
EOF

# Test 1: Try to link WITHOUT stubs path (expected to fail, we allow it)
RUN echo "=== Test 1: Link without stubs (should fail) ===" && \
    nvcc /tmp/link_test.cu -o /tmp/link_test_noflags 2>&1 || \
    echo "Expected failure confirmed"

# Test 2: Link WITH proper stubs path and flags (MUST succeed)
RUN echo "=== Test 2: Link WITH stubs (MUST succeed) ===" && \
    nvcc /tmp/link_test.cu -o /tmp/link_test_withflags \
    -L/usr/local/cuda-12.1/targets/x86_64-linux/lib/stubs \
    -lcuda && \
    echo "✓ PREFLIGHT PASSED: Linker can find CUDA symbols"

# If we reach here, linking works. This stage acts as a gate for the expensive wheel-builder stage.

# ==============================================================================
# STAGE 2: WHEEL-BUILDER - Build llama-cpp-python with CUDA
# ==============================================================================
FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04 AS wheel-builder

LABEL stage="wheel-builder"
LABEL description="Build llama-cpp-python==0.2.90 with CUDA support for GTX 1660 Ti (CC 7.5)"

# Copy preflight verification (forces preflight to complete first)
COPY --from=preflight /tmp/link_test_withflags /tmp/preflight_passed

# Install build dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    python3.10-dev \
    build-essential \
    cmake \
    ninja-build \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip and install build tools
RUN python3.10 -m pip install --upgrade pip setuptools wheel

# Set CUDA environment variables for build
ENV CUDA_HOME=/usr/local/cuda-12.1
ENV CUDA_PATH=/usr/local/cuda-12.1
ENV PATH=/usr/local/cuda-12.1/bin:${PATH}

# CRITICAL: Set CMAKE_ARGS to pass linker flags to find CUDA stubs
# This is what fixes the "undefined reference" errors
ENV CMAKE_ARGS="\
-DGGML_CUDA=on \
-DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.1/bin/nvcc \
-DCMAKE_CUDA_ARCHITECTURES=75 \
-DCMAKE_EXE_LINKER_FLAGS=-L/usr/local/cuda-12.1/targets/x86_64-linux/lib/stubs \
-DCMAKE_SHARED_LINKER_FLAGS=-L/usr/local/cuda-12.1/targets/x86_64-linux/lib/stubs \
-DCMAKE_MODULE_LINKER_FLAGS=-L/usr/local/cuda-12.1/targets/x86_64-linux/lib/stubs"

# Additional environment variables for CUDA build
ENV FORCE_CMAKE=1
ENV CUDACXX=/usr/local/cuda-12.1/bin/nvcc
ENV GGML_CUDA=1
ENV CUDA_DOCKER_ARCH=75

# Create wheel output directory
RUN mkdir -p /wheels

# Build llama-cpp-python wheel with CUDA support
# This will take ~20 minutes but should NOT fail at link stage thanks to our CMAKE_ARGS
RUN echo "=== Building llama-cpp-python==0.2.90 with CUDA support ===" && \
    echo "Target: GTX 1660 Ti (Compute Capability 7.5)" && \
    echo "CMAKE_ARGS: ${CMAKE_ARGS}" && \
    export LD_LIBRARY_PATH=/usr/local/cuda-12.1/targets/x86_64-linux/lib/stubs:$LD_LIBRARY_PATH && \
    pip wheel --no-cache-dir --wheel-dir=/wheels llama-cpp-python==0.2.90 && \
    echo "✓ Wheel built successfully" && \
    ls -lh /wheels/

# Verify the wheel was created
RUN test -f /wheels/llama_cpp_python-*.whl || \
    (echo "ERROR: Wheel not found in /wheels/" && exit 1)

# ==============================================================================
# STAGE 3: RUNTIME - Final Production Container
# ==============================================================================
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04 AS runtime

LABEL maintainer="WhisperServer Production"
LABEL description="GPU-accelerated speech AI with NeMo ASR, Gemma/LLaMA, and FastAPI"
LABEL cuda.version="12.1.1"
LABEL python.version="3.10"

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    ffmpeg \
    libsndfile1 \
    curl \
    wget \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN python3.10 -m pip install --upgrade pip

# Set working directory
WORKDIR /app

# Copy requirements.txt
COPY requirements.txt /app/

# CRITICAL: Filter out llama-cpp-python from requirements.txt
# We will install it from our pre-built wheel instead
RUN grep -v "^llama-cpp-python" requirements.txt > /tmp/requirements_filtered.txt || true

# Install Python dependencies (excluding llama-cpp-python)
RUN pip install --no-cache-dir -r /tmp/requirements_filtered.txt

# Copy the CUDA-enabled wheel from wheel-builder stage
COPY --from=wheel-builder /wheels/*.whl /tmp/

# Install llama-cpp-python from our CUDA wheel
RUN pip install --no-cache-dir /tmp/llama_cpp_python-*.whl && \
    rm /tmp/llama_cpp_python-*.whl

# CRITICAL BUILD-TIME VERIFICATION: Ensure GPU offload is enabled
RUN python3.10 << 'PYEOF'
import sys
try:
    import llama_cpp
    version = llama_cpp.__version__
    gpu_support = llama_cpp.llama_supports_gpu_offload()
    
    print(f"llama-cpp-python version: {version}")
    print(f"GPU offload support: {gpu_support}")
    
    if not gpu_support:
        print("ERROR: llama-cpp-python does NOT have GPU support!")
        print("This should never happen. The wheel was built incorrectly.")
        sys.exit(1)
    
    print("✓ GPU VERIFICATION PASSED: llama_supports_gpu_offload() == True")
except Exception as e:
    print(f"ERROR during GPU verification: {e}")
    sys.exit(1)
PYEOF

# Copy application code
COPY src /app/src
COPY frontend /app/frontend

# Set runtime environment
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0
ENV PYTHONPATH=/app:$PYTHONPATH

# Expose ports
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run the application
CMD ["python3.10", "-m", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]

